\setchapterpreamble[u]{\margintoc}
\chapter{Introduction}
\labch{intro}

\textit{This project was born in the bar of a pub where the parents of RDF graph validation were talking about creating a tool that would allow people who were not computer-scientist to get started and later work with RDF graph validation schemes.}

Each day more and more devices generate data both automatically and manually, and also each day the development of application in different domains that are backed by databases and expose these data to the web becomes easier. The amount and diversity of data produced clearly exceeds our capacity to consume it.
To describe the data that is so large and complex that traditional data processing applications can’t handle the term big data has emerged. Big data has been described by at least three words starting by V: volume, velocity, variety. Although volume and velocity are the most visible features, variety is a key concern which prevents data integration and generates lots of interoperability problems.
In order to solve this key concept RDF was proposed as a graph-based data model which became part of the Semantic Web vision. Its reliance on the global nature of URIs offered a solution to the data integration problem as RDF datasets produced by different means can seamlessly be integrated with other data.
Also, and related to his is the concept of Linked Data \sidecite{heath2011linked} that was proposed as a set of best practices to publish data on the Web. It was introduced by Tim Berners-Lee and was based on four main principles. RDF is mentioned in the third principle as one of the standards that provides useful information. The goal of this principles is that data is not only ready for humans to navigate through but also for other agents, like computers, that may automatically process that data.
All the above motivations helped to make RDF the language for the Web of Data, as described in \sidecite{gayo2017validating}. And the main features that it presents are: Disambiguation, Integration, Extensibility, Flexibility and Open by Default. All this concepts will be deeply explored in the Section 3 but with the features also some drawbacks are associated, the most important one and the one we will focus is the RDF production/consumption dilema.
RDF production/consumption dilema states that it is necessary to find ways that data producers can generate their data so it can be handled by potential consumers. For example, they may want to declare that some nodes have some properties with some specific values. Data consumers need to know that structure to develop applications to consume the data.
Although RDF is a very flexible schema-less language, enterprise and industrial applications may require an extra level of validation before processing for several reasons like security, performance, etc.
To solve that dilema and as an alternative to expecting the data to have some structure without validation, Shape Expressions (ShEx) where proposed as a human-readable and high-level open source language for RDF validation. Initially ShEx was proposed as a human-readable syntax for OSLC Resource Shapes \sidecite{ryman2013oslc} but ShEx grew very fast to embrace more complex user requirements coming from clinical and library use cases.
Another technology, SPIN, was used for RDF validation, principally in TopQuadrant’s TopBraid Composer. This technology, influenced from OSLC Resource Shapes as well, evolved into both a private implementation and open source definition of the Shapes Constraint Language (SHACL), which was adopted by the W3C Data Shapes Working Group.

\section{Motivation}

From a user point of view the possibilities of ShEx are very large, from the smallest case to just validate a node with one property to a scientific domain case where we need to validate the human genome (a real use case of ShEx). Ass seen, ShEx is a new powerful language, but it can became complicated on the corner cases, but most of day-to-day uses can be solved with a subset of the language. This is the point where this project borns. We will call this subset ShEx-Lite. The simplicity of ShEx-Lite is not only focus on computer scientists who have experience the pain of new languages but also for other non-technical profiles that need to validate RDF data.
It might seem like there are not to many profiles that can be categorized as target profiles for ShEx-Lite but a perfect example that this is just not true si the Wikidata Community. Wikidata is formed by a multidisciplinar community whose aim is to introduce RDF data in to an open knowledge base used by other companies like Google Search. The only problem is that the introduced RDF data needs validation to ensure a minimum data quality, but the profiles that introduce the data, usually, are expert domains whose knowledge about computer science is limited.
Besides to this, a common problem is that some companies like Wikidata or even Universities use ShEx to define the constraints of the RDF data that they own. But then, when developing applications with object oriented languages they need to translate those schemas in to a domain model to support their data. Furthermore if the Shape Expressions used to validate their data change for some reason they need to rewrite that domain model in the OOL again.
Finally, from a ShEx developer point of view sometimes appears the need to try new features in a small playground that allow easy an fast testing, for example a feature that appeared after this project was implemented is to automatically generate documentation webpages for the schemas defined in ShEx, but the first target of this feature won’t be ShEx, will be ShEx-Lite as it is perfect for he proof of concept.

\section{Purpose}
The general idea would be that the purpose of this project is to solve the problems described in the motivation section, and in order to solve those problems two stages.
First, the design and implementation of a compiler for a language defined as a subset of the shape expressions language focused on helping the non-expert user on solving problems with their schemas. And, on the other hand, implement a functionality in this compiler, that allows to automatically create domain object models in object-oriented programming languages, from the defined schemas.

\subsection{Compiler design and implementation}
The purpose of the compiler is to define the subset of the shape expression language that allows expressing basic constraints. Once this set has been defined, design and implement a compiler through the "compiler as a library" paradigm.
This compiler must be able to parse a schema, analyze it and generate the syntactic and semantic errors that the schema contains. The generation of code that allows validating RDF graphs with the compiler input schemas is not part of the scope of this project.

\subsection{Automatic generation of domain object models}
As stated previously, a demanded functionality is the automatic generation of domain models for object-oriented languages. Therefore, the main functionality included in this compiler, apart from syntactic and semantic validations, is the generation of intermediate representations of domain models from schemas. This means that for any scheme the object or objects could be generated automatically in the chosen object-oriented language.

\section{Contents}
The project layout is as follows:\\

\begin{description}
	\item[Chapter 2] Indicates the state of the art of the existing RDF validation technologies, tools for processing Shape Expressions and other related projects.
	\item[Chapter 3] Describes the goals that the project aim to achieve after its execution and possible real-world applications.
	\item[Chapter 4] Contains a detailed initial planning and budget for the project, this is the designed planning followed during the execution of the project and the initial estimated budget.
	\item[Chapter 5] Gives a basic theoretical background that it is needed to fully understand the concepts explained in the following chapters.
	\item[Chapter 6] Provides a technical description of the design and implementation of the compiler itself. This includes, analysis, design, the technological stack choices, diagrams, implementation decisions and tests.
	\item[Chapter 7] Compares the initial planning developed in chapter 4 with the final one. This includes the genuine execution planning of the project and the reasons and events that modified the one from chapter 4.
	\item[Chapter 8] Summarizes the analysis and results given over the project, gives an outlook for future work continuing the development of the implemented solution. And includes the diffusion of results done during the project.
	\item[Chapter 9] Includes all the set of references used during this document. It is fully recommended to read them carefully and use them as source of truth for any doubt.
	\item[Chapter 10] Attaches every document related to the project and referenced from other chapters that has been developed during the project. Here we include detailed budget, system manuals, and other documents.
\end{description}
